\chapter{Background Technologies}
\label{chap:background-technologies}

Developing a scalable graph pattern matcher requires a wide range of technologies. Careful selection of the technologies is critical to the success of the project. For \iqd{}, we looked for technologies that can form the building blocks of a distributed, scalable model repository and pattern matcher. These technologies must been designed with scalability in mind and deployed in large-scale distributed systems successfully. %To avoid licensing issues, our search criteria included that, if possible, the technologies should be free and open-source solutions.

Usually, instance models are graph-like data structures. Therefore, we looked for scalable graph databases. In this context, scalability requires distributed storage and querying capabilities.

During the early phase of the research, we studied the architecture and limitations of the candidate systems. For databases, we inspected the data sharding strategies, consistency guarantees and transaction capabilities, along with the API and query methods. We also checked the support for asynchronous processing, notification and messaging mechanisms.

In this chapter, we introduce the concepts and technologies that can form the basis of a scalable, distributed, asynchronous system. %We also present EMF (Eclipse Modeling Framework) and \eiq{}, an incremental model query evaluation tool.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Big Data and the NoSQL Movement}
\label{nosql}

Since the 1980s, database management systems based on the relational data model~\cite{Codd:1970:RMD:362384.362685} dominated the database market. Relational databases have a number of important advantages: precise mathematical background, understandibility, mature tooling and so on. However, due to their rich feature set and the strongly connected nature of their data model, relational databases often have scalability issues~\cite{Jacobs:2009:PBD:1536616.1536632, Sakr13}. They are typically optimized for transaction processing, instead of data analysis (see \emph{data warehouses} for an exception). In practice, these render them impractical for a number of use cases, e.g.\ running complex queries on large data sets.

In the last decade, large organizations struggled to store and process the huge amounts of data they produced. This problem introduces a diverse palette of scientific and engineering challenges, called \emph{Big Data} challenges. 

Big Data challenges spawned dozens of new database management systems. Typically, these systems broke with the strictness of the relational data model and utilized simpler, more scalable data models. These systems dropped support for the SQL query language used in relational databases and hence were called \emph{NoSQL databases}\footnote{The community now mostly interprets NoSQL as ''not only SQL''.}~\cite{NoSQL}. Because relational databases are not suitble for large-scale model-driven applications, we experimented with numerous NoSQL databases.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Concepts}

This section introduces the most important concepts used in this report. %We shortly introduce the concept of \emph{metamodeling}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Graph Data Models}

Along the well-known and widely used relational data model, there are many other data models. NoSQL databases are often categorized based on their data model (e.g.\ key--value stores, document stores, column families). In this report, we focus on \emph{graph data models}. %For reference, we also mention the \emph{column family} data model, but ignore other popular data models, like \emph{key-value stores} and \emph{document stores}.

The graph is a well-known mathematical concept widely used in computer science. For our work, it is important to distinguish between different graph data models.

\picSmall{graph-classes}{Different graph data models~(based on \cite{DBLP:journals/corr/abs-1006-2361})}

The most basic graph model is the \emph{simple graph}, formally defined as $G = (V, E)$, where $V$ is the set of vertices and $E \subseteq V \times V$ is the set of edges. Simple graphs are sometimes referred as textbook-style graphs because they are an integral part of academic literature. Simple graphs are useful for modeling homogeneous systems and have plenty of algorithms for processing.

Simple graphs can be extended in several different ways (\figref{graph-classes}). To describe the connections in more detail, we may add directionality to edges (\emph{directed graph}). To allow different connections, we may label the edges (\emph{labeled graph}). 

\emph{Typed graph}s introduces types for vertices. \emph{Property graph}s (sometimes called \emph{attributed graphs}) add even more possibilites by introducing properties. Each graph element, both vertices and edges can be described with a collection of properties. The properties are key--value pairs, e.g. \texttt{type = 'Person'}, \texttt{name = 'John'}, \texttt{age = 34}. \emph{Semantic graph}s use URIs (Uniform Resource Identifiers) instead of labels, otherwise they have similar expressive power as labeled graphs. 

%Property graphs are powerful enough to describe Java objects or EMF instance models. 

%Itt sem logikus a sorrend. Először beszélj a tágabb fogalomról (graph data), és utána a metamodelingről. Így jobban érthetővé válik az is, hogy a következőkben miért az Ecore-ról kezdesz beszélni.
%Ennek megfelelően, a metamodelingnél azt kellene leírni, hogy miben több a metamodel a graph data modelnél. Hol helyezhető el a metamodeling a 2.1-es ábrán? (ezt szöveggel kéne megoldani, az ábra maradjon úgy ahogy van)

Graph models are found in many languages and environments. In the following, we will present the ones most important for this report: the Ecore metamodeling language, the TinkerPop framework and the Resource Description Framework (RDF).

\subsubsection{Metamodeling}

Metamodeling is a methodology for the definition of modeling languages. A metamodel specifies the abstract syntax (structure) of a modeling language. Metamodels are expressed using a metamodeling language that itself is a modeling language. The metamodel can also be interpreted as the object-oriented data model of the language under design. %Metamodeling allows us to define spefic graph models. In general, metamodeling enforces typing, resulting in \emph{typed graphs}, but also allows for attributes, thus, it can express \emph{property graphs} as well.
Metamodeling can be viewed as the grammar for a \emph{typed property graph}, so the created models are both \emph{typed graph}s and \emph{property graph}s. 

\subsubsection{Ecore}
\label{ecore}

\picSmall{ecore-kernel}{The Ecore kernel, a simplified subset of the Ecore metamodel}

Ecore is the metamodeling language used by EMF. It has been developed in order to provide an approach for metamodel definition that supports the direct implementation of models using a programming language. The main rationale in introducing Ecore separately that it is the \emph{de facto} standard metamodeling environment of the industry, and several domain-specific languages are defined using this formalism.

\autoref{fig:ecore-kernel} illustrates the core elements of the Ecore approach. The full metamodel can be found in the EMF documentation~\cite{Ecore}. The most important elements are the following.
 
\begin{itemize}
  \item \verb+EClass+ models classes (or concepts). \verb+EClass+es are identified by name and can have several attributes and references. To support inheritance, a class can refer to a number of \emph{supertype} classes.
  \item \verb+EAttribute+ models attributes, that contain data elements of a class. They are identified by name and have a \emph{data type}.
  \item \verb+EDataType+ is used to represent simple data types that are treated as atomic (their internal structure is not modeled). Data types are also identified by their name.
  \item \verb+EReference+ represents a unidirectional association between \verb+EClass+es and as identified by a name. Lower and upper multiplicities can be specified. It is also possible to mark a reference as a \emph{containment} that represents composition relation between elements. If a bidirectional association is needed, it should be modeled as two \verb+EReference+ instances that are mutually connected via their \emph{opposite} references.
\end{itemize}

The rest of the details of Ecore has been ommitted for the sake of clarity, see~\cite{Ecore} for further reference.

\subsubsection{TinkerPop framework}

\picTiny{tinkerpop-stack}{The TinkerPop software stack \cite{Blueprints}}

The \emph{TinkerPop} framework is an open-source software stack for graph storage and processing \cite{TinkerPop}. TinkerPop includes \emph{Blueprints}, a property graph model interface. Blueprints fulfills the same role for graph databases as JDBC does for relational databases. Most NoSQL graph databases implement the property graph interface provided by Blueprints, including Neo4j (\autoref{neo4j}), Titan (\autoref{titan}), DEX~\cite{DEX}, InfiniteGraph~\cite{InfiniteGraph} and OrientDB~\cite{OrientDB}.

TinkerPop also introduces a graph query language, \textit{Gremlin}. Gremlin is a domain-specific language based on Groovy, a Java-like dynamic language which runs on the Java Virtual Machine. Unlike most query languages, Gremlin is an imperative language with a strong focus on graph traversals. 

Gremlin is based on \textit{Pipes}, TinkerPop's dataflow processing framework. Besides traversing, Gremlin is capable of analyzing and manipulating the graph as well.

TinkerPop also provides a graph server (\textit{Rexster}), a set of graph algorithms tailored for property graphs (\textit{Furnace}) and an object-graph mapper (\textit{Frames}). The TinkerPop software stack is shown on \figref{tinkerpop-stack}.

\subsubsection{Resource Description Framework}

The Resource Description Framework (RDF) is a family of W3C (World Wide Web Consortium) specifications originally designed as a \emph{metadata data model}. 

The RDF data model is based on the idea of making statements about \emph{resources} in the form of triples. A triple is a data entity composed of a \emph{subject}, a \emph{predicate} and an \emph{object}, e.g.\ ''John instanceof Person'', ''John is 34''.  

Triples are typically stored in \emph{triplestores}, specialized databases tailored to store and process triples efficiently. Also, some triplestores are capable of \emph{reasoning}, i.e.\ inferring logical consequences from a set of facts or axioms. Triplestores are mostly used in semantic technology projects.

Triplestores are usually queried via the RDF format's query language, SPARQL (recursive acronym for SPARQL Protocol and RDF Query Language). 

The RDF data model is capable of expressing \emph{semantic graph}s. Although the semantic graph data model has less expressive power than the property graph data model, by introducing additional resources for each property, a property graph can be easily mapped to RDF.

\subsubsection{Mapping Ecore to Other Data Models}
\label{ecore-mapping}

Our intention to reuse \eiq{} for building \iqd{} required us to map EMF's metamodel, Ecore to the domain of property graphs and RDF models.

\begin{table}[htb]

\centering
\begin{tabular}{ | l | l | l | }

\hline
\bf Ecore concept          & \bf Property graph concept  & \bf RDF concept \tabularnewline \hline\hline
\verb+EClass+ instance     & nodes' \verb+type+ property & \verb+rdfs:Resource+ \\ \hline
\verb+EAttribute+ instance & nodes' property names       & \verb+rdf:Property+  \\ \hline
\verb+EReference+ instance & edge label                  & \verb+rdf:Property+  \\ \hline
\verb+EDataType+ instance  & Java primitive types        & \verb+rdfs:Datatype+ \\ \hline
 
\end{tabular}
\caption{Mapping Ecore to property graphs and RDF}
\label{tab:ecore-mapping}

\end{table}




\subsection{Sharding}
 
To provide scalable persistence and processing for large amounts of data, the data has to be split between multiple computers. This process is known as \emph{data sharding}. \emph{Graph sharding} is a particularly difficult problem due to the strongly connected and mutable nature of graphs. Efficient sharding of graphs is still an open research area~\cite{ShardingGraph}.

\pic{graph-sharding}{Different partitionings of the same graph}

To illustrate the problem, \figref{graph-sharding} shows different partitionings of the same graph in a three-node cluster.
In case \textcircled{1} most edges run between servers and are therefore expensive to traverse.
In case \textcircled{2}, \emph{Server 2} is overloaded, taking more than three quarters of the total load.
Case \textcircled{3} presents a more balanced sharding of the graph.
Unfortunately, for large graphs, balanced sharding is hard to achieve in practice.

Most graph partition problems NP-hard and practical solutions to these problems could be derived using heuristics and approximation algorithms~\cite{conf/stacs/FeldmannF12}. Unfortunately, open-source database implementations lack support for such algorithms.


\subsection{Query Languages and Evaluation Strategies}

In the context of this report, a query defines a \emph{graph pattern}. The result of the query is a \emph{set of subgraphs} of the original graph. Graph patterns are useful for identifying patterns in a set of connected data elements. They are especially widely used in the context of model-driven engineering for defining well-formedness validation constraints and graph transformations.

\subsubsection{Query Language}

Queries can be defined in both imperative and declarative languages. The theoretical basis for most declarative query languages is first order logic. Both tuple relational calculus and relational algebra (widely used in query processing) are offshoots of first-order logic.

As mentioned earlier, unlike most query languages, Gremlin is an imperative query languages describing graph traversals. For example, if John's father is Jack and Jack's father is Scott, we may run the traversals shown on \lstref{gremlin-queries}.

\begin{lstlisting}[caption=Simple Gremlin queries, label=lst:gremlin-queries]
gremlin> g.V('name', 'John').out('father')
==>Jack
gremlin> g.V('name', 'John').out('father').out('father')
==>Scott
\end{lstlisting}

\subsubsection{Query Evaluation Strategies}

Query engines can be divided into two core categories: \emph{search-based} and \emph{incremental} engines. The main difference between these approaches is the way they reevaluate queries. While search-based engines process the whole data set (i.e.\ not just the data elements affected by the change), incremental engines utilize some data structures to be able to reevaluate the query based on the change set.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Graph Storage Technologies}
\label{graph-storage-technologies}

In this section, we compare different graph storage technologies by systematically discussing their \emph{architecture} and \emph{data model}. We inspect their \emph{sharding} strategies for distributed storage.
We also present their \emph{query languages and evaluation strategies}, with particular emphasis on the support of distributed operations. 

%---------------------------------------------------------------------------------------------------

\subsection{EMF Technologies}
\label{emf}

Eclipse is a free, open-source software development environment and a platform for plug-in development. Eclipse comes with its own modeling technologies called EMF (Eclipse Modeling Framework). EMF's primary goals are application design and code generation.

\subsubsection{Architecture}

EMF models can be persisted as XMI (XML Metadata Interchange) documents. By design, EMF models cannot be fragmented, i.e.\ they can only be used if they fit to a computer's main memory. There are different model repositories and persistence frameworks which can handle large EMF models~\cite{Scheidgen:2012:ATM:2404962.2404974}. 

\begin{itemize}
  \item CDO (Connected Data Objects), a distributed shared model framework for EMF models and metamodels~\cite{CDO}. CDO provides an object-relational mapping from Ecore to databases.
  \item Morsa~\cite{Pagan:2011:MSA:2050655.2050665} is a distributed model repository based on MongoDB~\cite{MongoDB}, a popular NoSQL database management system.
\end{itemize}
 
\subsubsection{Data Model}

EMF uses the Ecore data model, discussed in \autoref{ecore}. 

\subsubsection{Sharding}

Due to the nature of XML documents, EMF models serialized to a single XMI document cannot be sharded. CDO does not support automatic sharding, however Morsa does so by using MongoDB's sharding mechanism.

\subsubsection{Query Language and Evaluation}
\label{emf-query-language}

\paragraph{OCL} OCL (Object Constraint Language) is a declarative query language % a formal language used  
to describe well-formedness constraints on UML models. These expressions typically specify invariant conditions that must hold for the system being modeled or queries over objects described in a model. %Eclipse OCL is implemented with a \emph{local-search based}

\paragraph{\eiq{}} \eiq{}~\cite{icmt2011} is an Eclipse project developed by the Fault Tolerant Systems Research Group in the Budapest University of Technology and Economics. It provides IQPL (\iq{} Pattern Language), a declarative language to express queries over EMF models in the form of graph patterns. With the language the user can express combined queries, negative patterns, checking property conditions, simple calculations, calculate disjunctions and transitive closures, etc.\ on top of the models. The goal of \eiq{} is to provide \emph{incremental query evaluation}.

Our research work builds on \eiq{}, both in theory and practice. We used the Rete algorithm (\autoref{rete}) which allowed us to reuse some of the existing code base. We also utilized the methodology and environment, originally used to benchmark \eiq{} (\autoref{trainbenchmark}).

%---------------------------------------------------------------------------------------------------

\subsection{Neo4j}
\label{neo4j}

Neo4j, developed by Neo Technology, is the most popular NoSQL graph database. Neo4j is one of the most mature NoSQL databases. It is well documented and provides ample tooling, including an Eclipse-based visualization application, Neoclipse~\cite{Neoclipse}.

\subsubsection{Architecture}

Neo4j can be deployed in two scenarios. In \emph{embedded mode}, it runs in the same JVM (Java Virtual Machine), as the client application. In this setup, the database cannot be accessed by other applications. In \emph{server mode}, the database can serve requests from multiple clients over a REST (Representational State Transfer) interface.

\subsubsection{Data Model}

Neo4j implements the TinkerPop framework's Blueprints property graph data model. Neo4j is capable of loading graphs from \graphml{}~\cite{GraphML} and Blueprints \graphson{}~\cite{BlueprintsGraphSON} formats (see \autoref{property-graph-formats} for examples). 

\subsubsection{Sharding}

Instead of sharding, Neo4j only supports replication of data to create a highly available cluster. This implies serious scalability limitations to the system. Neo4j's developers make serious efforts to improve the scalability of the database in an ongoing project called Rassilon~\cite{rassilon}.

\subsubsection{Query Language and Evaluation}

Neo4j can be queried in various ways. When deployed in embedded mode, the application can use its Java-based core API. In both embedded and server mode, Neo4j provides two query languages. The first is the TinkerPop framework's imperative Gremlin language, primarily targeted for graph traversals. The second is Neo4j's own declarative query language for graph pattern matching, Cypher. 

%---------------------------------------------------------------------------------------------------

\subsection{Titan}
\label{titan}

Titan is a distributed, scalable graph database from Aurelius, the creators of the TinkerPop framework. To understand Titan's complex architecture, we present two additional concepts: the \emph{MapReduce} paradigm and the \emph{column family} data model.

\paragraph{Asynchronous Parallel Processing with MapReduce} 

The \emph{MapReduce paradigm} defines a parallel, asynchronous way of processing the data. As the name implies, MapReduce consists of two phases: the \emph{map} function processes each item of a list. The resulted list is then aggregated by the \emph{reduce} function. MapReduce is often used for sorting, filtering and aggregating data sets. It is also used for fault-tolerant, distributed task execution.

\paragraph{The Column Family Data Model}

A column family is similar to a table of a relational database: it consists of rows and columns. However, unlike in a relational database's table, the rows do not have to have the same fixed set of columns. Instead, each row can have a different set of columns. This makes the data structure more dynamic and avoids the problems associated with NULL values.

\subsubsection{Architecture}

Titan is not a standalone database, instead, it builds on top of existing NoSQL database technologies and leverages Hadoop's MapReduce capabilities. Titan supports various storage backends, including Cassandra and HBase. In the following, we shortly cover the technologies Titan builds upon. Both Titan and its dependencies are open-source software, written in Java. 

\paragraph{Hadoop}

Hadoop is a distributed data processing framework inspired by Google's publications about MapReduce~\cite{Dean:2008:MSD:1327452.1327492} and the Google File System~\cite{Ghemawat:2003:GFS:945445.945450}. Originally developed at Yahoo!, Hadoop is now an Apache project~\cite{Hadoop}. Like Google's systems, Hadoop is designed to run on commodity hardware, i.e.\ server clusters built from commercial off-the-shelf products. Hadoop provides a distributed file system (HDFS) and a column family database (HBase). %All software in the Hadoop framework is written in Java. 
A typical Hadoop cluster consists of a single master node which is responsible for the coordination of the cluster and worker nodes which deal with the data processing. The MapReduce job is coordinated by the master's \emph{job tracker} and processed by the slave nodes' \emph{task tracker} modules (\figref{hadoop-architecture}).

\picTiny{hadoop-architecture}{Hadoop's architecture \cite{HadoopMultinode}}

\paragraph{HDFS}

The Hadoop Distributed File System (HDFS) is an distributed file system, inspired by the Google File System and written specifically for Hadoop~\cite{Hadoop}. Unlike other distributed file systems (e.g.\ Lustre~\cite{Lustre}), which require expensive hardware components, HDFS was designed to run on commodity hardware. HDFS tightly integrates with Hadoop's architecture (\figref{hadoop-architecture}).% (\figref{hdfs}). 
The \mbox{\emph{NameNode}} is responsible for storing the metadata of the files and the location of the replicas. The data is stored by the \mbox{\emph{DataNode}s}.

%\picSmall{hdfs}{HDFS' architecture}

\paragraph{HBase} 

HBase~\cite{HBase} is an distributed column family database. It is developed as part of the Hadoop project and runs on top of HDFS. The tables in an HBase database can serve as the input and the output for MapReduce jobs run in Hadoop.

\paragraph{Cassandra}

Cassandra is one of the most widely used NoSQL databases~\cite{Cassandra}. Originally developed by Facebook~\cite{Lakshman:2010:CDS:1773912.1773922}, Cassandra is now an Apache project. 
Cassandra's a column family database with advanced fault-tolerance mechanisms. It allows the application to balance between availability and consistency by allowing it to tune the consistency constraints. Cassandra is used mainly by Web 2.0 companies, including Digg, Netflix, Reddit, SoundCloud and Twitter. It is also used for research purposes at CERN and NASA \cite{CassandraCompanies}.

\subsubsection{Data Model}

\picSmall{titan-cassandra-mapping}{Graph vertex mapped by Titan to a row in a Cassandra database}

To store the graph, Titan maps each vertex to a row of a column family (\figref{titan-cassandra-mapping}). The row stores the identifer and the properties of the vertex, along both the incoming and outgoing edges' identifiers, labels and properties.

\subsubsection{Sharding}

Titan uses the storage backend's partitioner, e.g.\ Cassandra's hash-based RandomPartitioner to shard the data. A more sophisticated partitioning system that will allow for partitioning based on the graph's static and dynamic properties (its domain and connectivity, respectively) is under implementation as of October 2013, but not yet available.

\subsubsection{Query Language and Evaluation}

Titan supports the TinkerPop framework's Gremlin query language. Gremlin/Pipes utilizes a depth-first search algorithm.

\paragraph{Faunus}

Although Titan was designed with scalability in mind, its query engine does not work in a parallel way. Also, it is unable to cope with queries resulting in millions of graph elements. To address this shortcoming, Aurelius developed a Hadoop-based graph analytics engine, Faunus. Faunus has its own format called Faunus \graphson{}. The Faunus \graphson{} format is vertex-centric: each row represents a vertex of the graph. This way, Hadoop is able to efficiently split the input file and parallelize the load process. See \autoref{faunus-graphson-example} for an example. Unlike the Gremlin implementation in Neo4j and Titan, the implementation in Faunus is based on breadth-first search. It is important to note that Faunus always traverses the whole graph and does not use its indices. This makes retrieving nodes or edges by type very slow (see our typical workload in \autoref{indexing}).

%---------------------------------------------------------------------------------------------------

\subsection{4store}
\label{4store}

4store is an open-source, distributed triplestore created by Garlik~\cite{4store}. Unlike the other tools discussed earlier, 4store is written in C. 4store is primarily applied for semantic web projects.

\subsubsection{Architecture}

4store was designed to work in a cluster with high-speed networks. 4store server instances are capable of discovering each other using the Avahi configuration protocol \cite{Avahi}. 4store offers a command-line and a HTTP server interface. 

\subsubsection{Data Model}

4store's data model is an RDF graph. It supports RDF/XML input format, which is processed using the Raptor RDF Syntax Library~\cite{Raptor}.

\subsubsection{Sharding}

Similar to Titan's partitioning, 4store's \emph{segmenting} mechanism distributes the RDF resources evenly across the cluster. 4store also supports replication by \emph{mirroring} tuples across the cluster.

\subsubsection{Query Language and Evaluation}

4store supports SPARQL queries with the Rasqal RDF Query Library~\cite{Rasqal}.  

%---------------------------------------------------------------------------------------------------

\subsection{Overview and Evaluation of Graph Storage Technologies}

\begin{table}[htb]

\centering
\begin{tabular}{ | l | l | m{2cm} | l | l | m{2cm} | }

\hline
\bf Technology & 
\bf Data model & 
\bf Distributed operation  & 
\bf Sharding   & 
\bf Queries    & 
\bf Identifier generation \tabularnewline \hline\hline
EMF     & Ecore           & Differs             & Differs             & OCL, IQPL & \textbf{Automatic}  \\ \hline
4store  & RDF             & Manual              & \textbf{Automatic}  & SPARQL    & Manual              \\ \hline
Neo4j   & Property graph  & Manual              & Manual              & Cypher    & Manual              \\ \hline
Titan   & Property graph  & \textbf{Automatic}  & \textbf{Automatic}  & Gremlin   & \textbf{Automatic}  \\ \hline

\end{tabular}
\caption{Overview of database technologies}
\label{tab:database-technologies}

\end{table}

\autoref{tab:database-technologies} summarizes the relevant characteristics of the aforementioned database management systems. These characteristics are crucial for building a distributed pattern matcher. According to these, Titan provides the most complete feature set. 4store and Neo4j lack important features like automatic identifier generation, which has to be implemented in the client application. Neo4j also misses automatic sharding, which seriously hinders its scalability potential. EMF's distributed operation and sharding capabilities depend on the actual model repository and database backend being used.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Building Scalable Asynchronous Distributed Systems: Akka}
\label{akka}

Most distributed, concurrent systems use a messaging framework or message queue service. %Because of the nature of the Rete algorithm (\autoref{rete}), 
The \iqd{} system also requires a distributed, asynchronous messaging framework. For this purpose, we used the Akka framework.

Akka is an open-source, fault-tolerant, distributed, asynchronous messaging framework developed by Typesafe \cite{Akka}.
Akka is implemented in Scala, a functional and object-oriented programming language which runs on the Java Virtual Machine. Akka provides language bindings for both Java and Scala.

\picSmall{akka-remote-deployment}{Deploying a remote actor in Akka \cite{Akka}}

Akka is based on the actor model \cite{Hewitt:1973:UMA:1624775.1624804} and provides built-in support for remoting. Unlike traditional remoting solutions, e.g.\ Java RMI (Remote Method Invocation) and CORBA (Common Object Request Broker Architecture), the remote and local interface is the same for each actor. Actors have both a logical and a physical path (\figref{akka-remote-deployment}). This way, they can be transparently moved between machines on the network.

As of October 2013, the latest version (Akka 2.2) also supports \textit{pluggable transport} to use various transports to communicate with remote systems \cite{Akka}. For serializing the messages, Akka supports different frameworks, including Java's built-in serialization, Google Protobuf \cite{Protobuf} and Apache Thrift \cite{Thrift}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\section{Eclipse-based Technologies}
% Members of the Eclipse Foundation include industry giants like IBM, Intel, Google and SAP.

%\iqd{}'s single workstation predecessor, \eiq{} is built around Eclipse-based technologies. To reap the benefits of a mutual code base, we designed \iqd{} to use as much of \eiq's components as possible. In the following section, we introduce the Eclipse-based technologies most important for our work.

% The Resource Description Framework (RDF~\cite{website:rdf_standard}) is developed to support the description of instances of the semantic web, assuming sparse, ever-growing, incomplete data. Semantic models are built up from triple statements and they can be queried using the SPARQL~\cite{SPARQL} graph pattern language with tools like Sesame~\cite{sesame} or Virtuoso~\cite{openvirtuoso}.
% 
% Property graphs~\cite{DBLP:journals/corr/abs-1006-2361} provide a more general way to describe graphs by annotating vertices and edges with key-value properties.
% 
% Such data structures can be stored in graph databases like Neo4j~\cite{neo4j} which provides the Cypher~\cite{cypher} query language. Even though big data storage (usually based on MapReduce) provides fast object persistence and retrieval, query engines realized directly on these data structures do not provide dedicated support for incremental query evaluation.

%\paragraph{Deployment}
%Titan can be deployed in different ways according to the needs of the application. For \iqd{}'s prototype, we used Titan in \textit{remote server mode} (\figref{titan-modes-distributed}). In this setup, Titan runs in the same Java Virtual Machine as the application and communicates with the Cassandra cluster on a low-level protocol (e.g.\ Thrift).
%\pic{titan-modes-distributed}{Using Titan with Cassandra in remote server mode}



%Ezt inkább úgy mondanám, hogy a metamodellezés általában egyesíti magában a típusosságot és a property-ket (ez utóbbiakat is típusrendszerbe igazítva), és ennek az Ecore egy konkrét megvalósítása.



%Itt sem logikus a sorrend. Először beszélj a tágabb fogalomról (graph data), és utána a metamodelingről. Így jobban érthetővé válik az is, hogy a következőkben miért az Ecore-ról kezdesz beszélni.

%Ennek megfelelően, a metamodelingnél azt kellene leírni, hogy miben több a metamodel a graph data modelnél. Hol helyezhető el a metamodeling a 2.1-es ábrán? (ezt szöveggel kéne megoldani, az ábra maradjon úgy ahogy van)
