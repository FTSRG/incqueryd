\chapter{Evaluation of Performance and Scalability}
\label{chap:evaluation}

We developed a prototype of \iqd{} to evaluate the feasibility of the approach. In the following chapter, based on the work for \eiq{}, we introduce a distributed performance benchmark. We present the benchmark environment and analyze the results, with particular emphasis on the scalability of our approach. 

% of measuring certain scalability aspects. We .
% we introduce the a distributed benchmark \tb{}, a performance benchmark for query engines and the distributed \tb{}, which is also capable

The prototype of \iqd{} is based on the architecture presented in \autoref{chap:overview}. A working prototype is beneficial for a number of reasons. First, it serves as a proof concept by demonstrating that a distributed, incremental pattern matcher is feasible with the technologies currently available.
On the other hand, it gives us the opportunity to define and run benchmarks, so that we can evaluate the scalability aspects of the system.

\section{Dimensions of Scalability}

A distributed system's \emph{scalability} has multiple dimensions. Usually, when aiming for \emph{horizontal scalability}, the most emphasized dimension is the \emph{number of processing nodes} (computers) in the system. However, there are other important aspects that include \emph{local resources} of the servers, \emph{network communication overhead}, etc. The main goals of our benchmark was to measure the scalability of \iqd{} with respect to the model size and compare it to other non-incremental query technologies. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Foundations: the \tb{}}
\label{trainbenchmark}

The \tb{} was designed at the Fault Tolerant Systems Research Group \cite{high-performance-queries, ASE2013} to measure the efficiency of model queries and manipulation operations in different tools. The \tb{} is primarily targeted for typical MDE workloads, more specifically for well-formedness validations.

\subsection{Benchmark Goals}

The \tb{} measures the response time. The benchmark models a ''real-world'' MDE workload by simulating a user's interaction with the model. In this sequence, the user loads the model and validates it against a set  queries (defining well-formedness constraints). The user edits the model in small steps. The user's work is more productive and less error-prone if she receives the results of the validation instantly after each edit. Therefore, the user would like to run re-evaluate well-formedness queries quickly.

\pic{trainbenchmark-sequence}{The \tb{}'s sequence}%. The green arrows and components are specific to the incremental scenario.}

The benchmark defines four distinct phases, also shown on \figref{trainbenchmark-sequence}.

\begin{enumerate}
  \item \emph{Load:} load the serialized instance model to the database \textcircled{1}.
  \item \emph{First validation:} execute the well-formedness query on the model \textcircled{2}.
  \item \emph{Transformation:} modify the model \textcircled{3}.
  \item \emph{Revalidation:} execute the well-formedness query again \textcircled{4}.
\end{enumerate}

To assess the scalability of the tools, the benchmark uses instance models of growing sizes, each model containing about twice as many model elements as the previous one (\autoref{trainbenchmark-model-generation}). Running the same validation sequence on different model sizes highlighted the limitations of the tested query engines.

Scalability is also evaluated against the complexity of the queries. The benchmark defines different queries, each testing different aspects of the query engine (filtering, join and antijoin operations, etc.). To achieve a successful run, the tested tool is expected to evaluate the query and return the \emph{identifier}s of the model elements in the result set.

\subsection{Generating Instance Models}
\label{trainbenchmark-model-generation}

Due to both confidentiality and technical reasons, it is difficult to obtain real-world industrial models and queries. Also, using confidential data sets hinders the reproducibility of the conducted benchmarks. Therefore, a generator modules was developed which creates instance models which mimic real-world models.

We used  the \emph{railway system metamodel}, defined in \autoref{overview-elaboration}. The instance models are generated pseudorandomly, with pre-defined structural constraints and a regular fan-out structure (i.e.\ nodes of a given type have similar indegree and outdegree)~\cite{ASE2013}. The generator is capable of generating models of different sizes and formats, including EMF, OWL, RDF and SQL. We also developed a generator for the property graph data model. In \autoref{trainbenchmark-mapping}, we some examples about mapping the Ecore kernel to property graphs.

\subsection{Original Results for Non-distributed Tools}

\pic{trainBenchmark_IncQuery_RouteSensor}{\tb{}: reponse times for incremental query evaluation, measured on a single node \cite{TODO}}

The \tb{} was designed to work with different tools originating from various technological spaces, e.g.\ EMF-based tools (\eiq{}, Eclipse OCL), semantic web technologies (AllegroGraph, Sesame), NoSQL databases (Neo4j), etc.

\figref{trainBenchmark_IncQuery_RouteSensor} shows the incremental transformation and validation time for the \emph{RouteSensor} query, discussed in \autoref{railroad-system}. The results clearly show the advantage of incremental query engines. Both Eclipse OCL Impact Analyzer and \eiq{} scale very well (their characteristic is almost constant to the model size and linear to the size of the result set), while non-incremental tools scale linearly at best, which renders them inefficient for lange models.

% The $x$ axis shows the models size (number of model elements), while the $y$ axis shows execution time. Both axes are logarithmic.

\pic{trainBenchmark_Memory_User_RouteSensor}{\tb{}: memory consumption of the tools \cite{TODO}}

\figref{trainBenchmark_Memory_User_RouteSensor} show the memory consumption of the diffent tools.


% see also http://www.inf.mit.bme.hu/sites/default/files/materials/category/kateg%C3%B3ria/oktat%C3%A1s/msc-t%C3%A1rgyak/szolg%C3%A1ltat%C3%A1sbiztons%C3%A1gra-tervez%C3%A9s-laborat%C3%B3rium/10/02_szolgbiztlab_teljesitmenyteszteles-segedlet.pdf

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Distributed \tb{}}
\label{distributed-trainbenchmark}

% Fogalmazz pontosabban: ebben a mérésben a hw archtektúra (tehát az elosztott rendszer elrendezése, és erőforrása) konstans végig, tehát kizárólag a modellméret növekedése függvényében vizsgáljuk a skálázhatóságot.
% A benchmark performance része pedig az, hogy a saját technológiát teljesítményét többféle baseline-nal is összeméred.

Based on the \tb{}, discussed in \autoref{trainbenchmark}, we created an extended version for distributed systems. The main goal of the distributed \tb{} is the same as the original's: measure the reponse time and reason about the scalability of different tools.

Another important goal is to compare the performance of the system to non-incremental query technologies.

\subsection{Distributed Architecture}

The distributed benchmark defines the same phases as the original \tb{}. The benchmark is controlled by a distinguished node of the system, called the \emph{coordinator}. The coordinator delegates the operations (e.g.\ loading the graph) to the distributed system. The queries and the model manipulation operations are handled by the database management system which runs them distributedly and waits for the distributed operation to finish (effectively creating a synchronization point after each one). 

\subsection{Benchmark Limitations}

It is possible that the incoming data sets lack a globally unique identifier. In this case, we need to automatically generate unique identifiers. While some systems, (e.g.\ Titan) support this, other systems (e.g.\ 4store) do not have such feature. For these systems, the \iqd{} middleware should be able to generate unique identifiers. This feature is subject to future work \autoref{future-work}.

A common reason for designing and implementing distributed systems is that they are capable of handling a large number of concurrent requests. This way, more users can use the system at the same time. In the distributed \tb{}, the system is only used by a single user. Simulating multiple users and issuing concurrent requests is subject to future work (\autoref{future-work}).

\subsection{Generating Instance Models}

For Neo4j, we already expanded the the generator with a \emph{property graph generator} module. The generator creates a graph in a Neo4j database and uses the Blueprints library's \texttt{GraphMLWriter} and \texttt{GraphSONWriter} classes to serialize it to \graphml{} (\autoref{graphml-example}) and Blueprints \graphson{} (\autoref{blueprints-graphson-example}) formats. 

Titan's Faunus framework requires a specific format called Faunus \graphson{} (\autoref{faunus-graphson-example}). To use Faunus, we extended the property graph generator to generate Faununs \graphson{} files as well.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Benchmark Environment}
\label{sec:benchmark-environment}

We used the distributed \tb{} (\autoref{distributed-trainbenchmark}) to evaluate \iqd{}'s performance and compare it to non-incremental solutions. In the following section, we will discuss the benchmark setup and the environment in detail. 

\subsection{Benchmark Setup}

We tested \iqd{} with three storage backends: first with Neo4j, then with 4store (\autoref{4store}) and Titan (\autoref{titan}). In both cases, the system was deployed on a four-node cluster.

As a \emph{non-incremental baseline}, we used Neo4j's and 4store's own query engine. While we also planned to use Titan, our experiments showed that even for medium-sized graphs, the system was unable to run even the elementary queries (e.g.\ retrieving vertices by type), not to mentioned the more complex ones.

The benchmark follows the phases defined in the distributed \tb{}. Note that the main difference between the batch and incremental scenarios is that the latter maintain a distributed Rete network, which allows efficient query (re)evaluation.

\subsection{Hardware and Software Ecosystem}
\label{ecosystem}

As the testbed, we deployed our system to a private cloud. The cloud is managed by Apache VCL (Virtual Computing Lab) and is also used for educational purposes. Therefore, during the benchmark, the network and the host machines could be under load from other users as well. We consider the effect of these in \autoref{threats-to-validity}.

The detailed configuration of the servers are provided below.

\subsubsection{Hardware}
 
Each virtual machine used two cores of an Intel Xeon L5420 CPU running at 2.5~GHz and had 8~GBs of RAM. The host machines were connected with gigabit Ethernet network connection.

\subsubsection{Software}

For the benchmarks, we used the following software stack. The technologies are discussed in \autoref{chap:background-technologies}.

\begin{itemize*}
  \item Ubuntu 12.10 64-bit
  \item Oracle Java 7 64-bit
  \item Neo4j 1.8
  \item 4store 1.1.5
  \item Titan 0.3.2
  \item Faunus 0.3.2
  \item Hadoop 1.1.2
  \item Cassandra 1.2.2
  \item Akka 2.1.2
\end{itemize*}

\subsection{Benchmark Methodology and Data Processing}
\label{benchmark-methodology}

Both during the development and in runtime we ensured the \emph{functional equivalence} of the measured tools. During the development, we followed the \tb{}'s well-defined specification~\cite{ASE2013}. This precisely defines the steps for each phase, e.g.\ the number of elements to modify in each transformation and the amount of transformation--validation cycles. In runtime, we checked the result set for correctness against the reference implementation. 

The benchmark coordinator software used the \tb{}'s framework to collect the data about the results of the benchmark. We measured the execution time of the predefined phases. The execution time includes the time required for the coordinator's operation, the computation and IO operations of the cluster's computers and the network communication (to both directions). The execution times were determined using Java's \texttt{System.nanoTime()} method.

The results were processed by an R script~\cite{RProject}. The script is capable of aggregating and visualizing the results. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Benchmark Results with Neo4j}
\label{benchmark-neo4j}

During the earlier phases of the research, we conducted measurement using only Neo4j. These results were published in \cite{Izso:2012:ODD:2428516.2428523}. The benchmark's setup was slightly different, with the main difference being that due to the lack of sharding in Neo4j, we \emph{sharded the graph manually}. This had some important implications.
  \begin{itemize}
    \item The \emph{batch} queries were ran on all shards separately and their results were aggregated by the coordinator. The transformations also ran separately. 
    \item The \emph{incremental} queries were evaluated with a distributed Rete network. The elementary model queries (for filling the indexers) were ran on all shards separately and aggregated by the indexers. The transformations also ran separately.
%    \item

  \end{itemize}
%  \item The servers had twice as much memory (16~GB).
%\end{itemize}

Because the graph was sharded to disjoint partitions with no edges between them, this can be viewed as an ideal case of graph sharding. Therefore, we can use the results to inspect the an ''ideal'' sharding strategy's impact on the performance. We present the most important results of the benchmark.

\pic{benchmark/neo/BatchTrafo_RouteSensor}{Total execution times for 50 validations}

%Despite the differences, we can still derive some useful conclusions by compare the results of this benchmark with the most recent one detailed previously.
\figref{benchmark/neo/BatchTrafo_RouteSensor} shows that \iqd{} with Neo4j consistently outperforms Neo4j's query engine.

\pic{benchmark/neo/OnTheFly_RouteSensor}{Execution times for transformation and revalidation}

\figref{benchmark/neo/OnTheFly_RouteSensor} shows that for transformation and revaliation, \iqd{} with Neo4j is about two orders of magnitude faster than Neo4j's query engine.


\section{Benchmark Results with 4store and Titan}
\label{evaluation-results}

This section presents the benchmark results with 4store and Titan. Unlike the benchmark with Neo4j (\autoref{benchmark-neo4j}), this benchmark used truly distributed storage backends.

% pictures not used
%\pic{benchmark/Read_RouteSensor}{Execution times for the $\mathit{Read}$ phase}
%\pic{benchmark/Check0_RouteSensor}{$\mathit{Check}_0$ phase}
%\pic{benchmark/OnTheFly_RouteSensor}{Execution times for transformation and revalidation} %($\mathit{Edit}$ and $\mathit{Check}_1$  phase)}

\pic{benchmark/BatchValid_RouteSensor}{Execution times for load and first validation}

The execution times for the \emph{load and first validation} phases are shown on \figref{benchmark/BatchValid_RouteSensor}.
As expected, due to the overhead of the Rete network's construction, the \emph{batch} tool is faster for small models. However, it is important to observe that even for medium-sized models (with a couple of million elements), the \iqd{} tools start to edge ahead. This shows that the Rete networkwork's construction overhead already pays off for the first validation.

\pic{benchmark/OnTheFlyEdit_RouteSensor}{Execution times for transformation}

The execution times for the \emph{transformation} phase are shown on \figref{benchmark/OnTheFlyEdit_RouteSensor}. The incremental tools provide faster transformation times due to the fact that instead of querying the database, the modeling application can rely on the query layer's indexers. Even for medium-sized models, the \iqd{} tools are more than two orders of magnitude faster than the batch tool.

\pic{benchmark/OnTheFlyCheck_RouteSensor}{Execution times of the revalidation}

The incremental tools have an even greater advantage for \emph{revalidation} times, shown on \figref{benchmark/OnTheFlyCheck_RouteSensor}. For medium-sized models, they are more than three orders of magnitude faster than the batch tool.

This shows that \iqd{} is not just capable of processing models with tens of millions of elements (well beyond the capabilities of single-node tools), but also, it provides sub-second revalidation times.

\pic{benchmark/BatchTrafo_RouteSensor}{Total execution times for 50 validations}

\figref{benchmark/BatchTrafo_RouteSensor} shows the total execution time for a sequence: loading the model, then running transformations and revalidations 50 times. Due to the large number of transformations and revalidations, incremental tools are significantly faster. For example, for a model with 6 million elements, the batch tool took almost 6 hours, while the 4store-based incremental tool took less than 5 minutes. 

\section{Result Analysis}

The results clearly show that the initialization of the Rete network adds some overhead during the \emph{load and first validation phases}. However, even for medium-sized models, this is easily outweighed by the high query performance of the Rete network.

The almost constant characteristic of the execution times of the \iqd{} tools' \emph{transformation} and \emph{validation} phases confirm that a distributed, scalable, incremental pattern matcher is feasible with current technologies. Based on the results, we can conclude that while network latency is present, the distributed Rete network still allows sub-second on-the-fly model validation operations. It is also important to observe the similar characteristic of \iqd{}'s and \eiq{}'s transformation and validation times (\figref{trainBenchmark_IncQuery_RouteSensor}).

Another important observation is that for \iqd{} tools, the execution time is approximately proportional to \emph{the size of the change}. For batch tools, it is proportional to the \emph{size of the model}. 

Note that these results and scalability characteristics do not apply for every workload profile. For example, if the user modifies large chunks of the model and issues queries infrequently, batch query evaluation methods can be faster. 

The high memory consumption of the Rete algorithm was one of our main motivations to build a distributed system. For very large models (beyond $10^8$ model elements, we ran into cases where the Java Virtual Machine ran out of or had just enough memory. This resulted in \texttt{OutOfMemoryError: Java heap space} and \texttt{OutOfMemoryError: GC overhead limit exceeded} exceptions, respectively. Introducing a \emph{Rete node sharding} or other fault-tolerance mechanisms for these cases is subject to future work (\autoref{future-work}).

The results show that the 4store-based \iqd{} prototype is consistently faster in the \emph{load} phase than the Titan-based one. This is due to 4store's simpler architecture and different data model, which is better suited to the \iqd{} middleware's elementary model queries.

Like the original \tb{}, we proved that incremental tools have an advantage for transformation and well-formedness validation sequences. Compared to the \tb{}, we managed to work with significantly larger models with more than 50 million model elements. Based on the results, we expect \iqd{} to also perform well on different data sets and queries.  

\section{Threats to Validity}
\label{threats-to-validity}

To guarantee the correctness of our benchmarks, we laid out some rules to ensure the precision of the results.

First, to start each benchmark sequence independently, we turned the operating system's caching mechanisms off. The execution time of the \emph{validation and transformation phases} were determined by running them 50 times and taking the \emph{median} values (we decided to take the median instead of the mean value, because the former is less sensitive to transient effects). This way, we could measure the Java Virtual Machine's warmup effect, which would also occur in a real-world model query engine running for several hours or even longer.

As discussed in \autoref{ecosystem}, our servers could be influenced from the workload caused by other users of the same cloud. To minimalize the effect of this and other transient loads, we ran the benchmark five times and took the \emph{minimum} value for each phase. We also disabled file caching in the operating system, so that the serialized model always must be read from the disk.

Despite our efforts, transient effects could still be present in the results. However, their effect is only a threat for smaller model sizes, where the measured execution times are low. For larger models with longer execution times, the transient effects do not threat the validity of the benchmark results.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Summary}

Our benchmarks proved that the proposed architecture is capable of providing scalable, incremental query evaluation.
\iqd{}'s scalability characteristics confirmed that despite the additional network latency, it is possible to keep \eiq{}'s almost constant performance characteristics in a distributed environment. The results show the model size barrier, primarily caused by limitations of memory, 
can be pushed further using a horizontal scaling approach.

It is important to note that our benchmark did not cover all aspects of distributed scalability. For example, simulating multiple users, measuring the exact memory consumption and network traffic of each server is subject to future work.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

