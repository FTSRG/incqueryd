\section{Results}
\label{evaluation-results}

In the following section, we discuss the results of our benchmark.

\subsection{Result visualizations}


%\pic{benchmark/Read_RouteSensor}{Execution times for the $\mathit{Read}$ phase}
%\figref{benchmark/Read_RouteSensor}

%\pic{benchmark/Check0_RouteSensor}{$\mathit{Check}_0$ phase}
%\figref{benchmark/Check0_RouteSensor}

The $x$ axis shows the models size (number of model elements), the $y$ axis shows execution time. Both axes are logarithmic. 

The execution times for the \emph{load and first validation} phases are shown on \figref{benchmark/BatchValid_RouteSensor}.
As expected, due to the overhead for the Rete network's construction, the \emph{batch} tool is faster for small models. However, it is important to observe that even for medium-sized models (with a couple of million elements), the network construction's overhead already pays off for the first validation.

%the $\mathit{Read}$ phase take about the same time for both scenarios, and \iqd{} is about half an order of magnitude slower when evaluating the query at first ($\mathit{Check}_0$ phase) due to the Rete construction overhead. However, \iqd{} is several orders of magnitude faster during the $\mathit{Edit}_i-\mathit{Check}_i$ cycles, making on-the-fly query (re)evaluation feasible even for models larger than 50 million elements. 

%4.5.1 prezentálni minimális magyarázattal (ami callout lenne), log scale, OOM differences

\pic{benchmark/BatchValid_RouteSensor}{Execution times for load and first validation}

The execution times for the \emph{transformation and revalidation} phases are shown on \figref{benchmark/OnTheFly_RouteSensor}. These results prove that the incremental approaches provide small and constant evaluation times. Even for medium-sized models, the incremental approaches are more than two orders of magnitude faster. For large models, the difference is even bigger.

\pic{benchmark/OnTheFly_RouteSensor}{Execution times for transformation and revalidation} %($\mathit{Edit}$ and $\mathit{Check}_1$  phase)}

%\pic{benchmark/OnTheFlyEdit_RouteSensor}{Execution times for transformation} %On-the-fly revalidation, $\mathit{Edit}$ phase}
%\figref{benchmark/OnTheFlyEdit_RouteSensor}
%\pic{benchmark/OnTheFlyCheck_RouteSensor}{Execution times of the revalidation} % ($\mathit{Check}_1$ phase)}
%\figref{benchmark/OnTheFlyCheck_RouteSensor}


\pic{benchmark/BatchTrafo_RouteSensor}{Total execution times for 50 validations}

\figref{benchmark/BatchTrafo_RouteSensor} shows the total execution time for a total sequence: loading the model, then running transformations and revalidations it, with a total of 50 validations. 

The results show that the 4store-based \iqd{} implementation is consistently faster than the Titan-based one. This is due to 4store's simpler architecture and different data model, which is better suited for the \iqd{} middleware's elementary model queries and modifications.

\section{Result Analysis}

% - összekapcsolni a látott eredményeket a rendszer felépítésével

The initialization of the Rete net adds some overhead during the \emph{load and first validation phases} (\figref{benchmark/BatchValid_RouteSensor}). However, even for medium-sized models, this is outweighed by the high query performance of Rete net.

The almost constant characteristic of execution times of the incremental tools \emph{transformation and validation} phases (\figref{benchmark/OnTheFly_RouteSensor}) confirm that a distributed, scalable, incremental pattern matcher is feasible with current technologies. The results also prove that while network latency is present, the distributed Rete network still allows quick on-the-fly model validation operations. 

An important observation is that for incremental tools, the execution time is approximately proportional to \emph{the number of affected model elements}. For non-incremental tools, it is proportional to the \emph{size of the model}. 

Once initialized, \iqd{} scales linearly, since query response times for growing models can be kept low by adding additional computers for hosting Rete nodes.

Note that these results and scalability characteristics do not apply for every workload profile. For example, if the user modifies large chunks of the model and issues queries infrequently, non-incremental query evaluation methods can be faster. 

%* inkrementális esetben a APPROX MÓDOSÍTÁSSAL arányos a lekérdezés ideje.
%* nem-inkrementális esetben a MODELLEL arányos
%* Rete akkor jó, ha kicsiket módosít a user

\subsection{Memory Consumption}

During our experiments, we measured cases where the Java Virtual Machine ran out of memory or had just enough memory, resulting in \texttt{OutOfMemoryError: Java heap space} and \texttt{OutOfMemoryError: GC overhead limit exceeded} exceptions, respectively. Introducing fault-tolerance mechanism for these cases is subject to future work (\autoref{future-work}).  
  



% \subsection{Results Using Neo4j}
% 
% \pic{benchmark/neo/BatchTrafo_RouteSensor}{Batch transformation}
% 
% \figref{}
% 
% \pic{benchmark/neo/OnTheFly_RouteSensor}{On-the-fly revalidation ($\mathit{Edit}$ and $\mathit{Check}_1$  phase)}
% 
% \figref{}

\subsection{Threats to validity}
\label{threats-to-validity}

4.5.3 threats to validity
  - befolyásoló tényezők
    - caching off
    - keresztbe terhelés, tranziens hibák 5x mérés, ...
