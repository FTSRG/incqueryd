\section{\tb{}}
\label{trainbenchmark}

The \tb{} was designed by Benedek Izsó, Zoltán Szatmári and István Ráth \cite{high-performance-queries} to measure the efficiency of model queries and manipulation operations in different tools. The \tb{} is primarily targeted for typical MDE workloads, more specifically for well-formedness validations.

The benchmark is built on the \emph{railway system metamodel}, defined in \autoref{overview-elaboration}.

\subsection{Benchmark Goals}

The \tb{} measures both the response time and the scalability of the tools. The benchmark models a ''real-world'' MDE workload by simulating a user's interaction with the model. In this sequence, the user loads the model and validates it against a set  queries (defining well-formedness constraints). The user edits the model in small steps. The user's work is more productive and less error-prone if she receives instant feedback after each edit. Therefore, we would like to run re-evaluate well-formedness queries quickly, which implies that incremental query engines are more favorable for this workload.

The benchmark defines four distinct phases:

\begin{enumerate}
  \item \emph{Load:} load the serialized instance model to the database.
  \item \emph{First validation:} execute the well-formedness query on the model.
  \item \emph{Transformation:} modify the model.
  \item \emph{Revalidation:} execute the well-formedness query again.
\end{enumerate}

To measure the scalability of the tools, the benchmark uses instance models of growing sizes, each model containing about twice as many model elements as the previous one (\autoref{trainbenchmark-model-generation}). Running the same validation sequence on different model sizes highlighted the limitations of the tested query engines.

Scalability is also measured along the complexity of the queries. The benchmark defines four queries, each testing different aspects of the query engine (filtering, join and antijoin operations, etc.).

\subsection{Results}

The \tb{} was implemented for different tools originating from various technological spaces, e.g.\ EMF-based tools (\eiq{}, Eclipse OCL), semantic web technologies (Allegro Graph, Sesame, 4store), NoSQL databases (Neo4j), etc.

\pic{classic-trainbenchmark-results}{\tb{} results measured on a single node}

\figref{classic-trainbenchmark-results} shows the incremental transformation and validation time for the \emph{RouteSensor} query, discussed in \autoref{railroad-system}. The results clearly show the advantage of incremental query engines. Both Eclipse OCL Impact Analyzer and \eiq{} scale sublinearly (their characteristic is almost constant), while non-incremental tools scale linearly at best, which renders them inefficient for lange models.

\subsection{Generating Models}
\label{trainbenchmark-model-generation}

Due to both confidentiality and technical reasons, it is difficult to obtain real-world industrial models and queries. Also, using confidential data sets hamstrings the reproducibility of the conducted benchmarks. Therefore, we generated instance models which mimic real-world models.

The instance models are generated pseudorandomly, with pre-defined structural constraints \cite{ASE2013}. The generator is capable of generating models of different sizes and formats, including EMF, OWL, RDF and SQL. 
