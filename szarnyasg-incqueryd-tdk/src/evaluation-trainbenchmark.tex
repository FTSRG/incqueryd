\section{Benchmark Scenario}

\label{benchmark}
In order to measure the efficiency of model queries and manipulation operations over the distributed architecture, we designed a benchmark to measure tool response times in a well-formedness validation use case. The benchmark transaction sequence consists of four phases: (i) during the $\mathit{load}$ phase, the serialization of the model is loaded into the database; (ii) a test query (\figref{patterndef}) is executed ($\mathit{check}_0$); finally, in a cycle consisting of several repetitions, some elements are programmatically modified ($\mathit{edit}_i$) and the test query is re-evaluated ($\mathit{check}_i$). We ran the benchmark on pseudo-randomly generated instance models of growing size, each model containing about twice as many elements (vertices and edges) as the previous one and having a regular fan-out structure. As the current version of Neo4j does not have built-in support for graph sharding, the benchmark uses a manually sharded strategy where each shard contains a disjoint partition of the model.

\pic{benchmark-baseline}{The non-incremental baseline benchmark's setup}

%TODO referencia leirasa (miert az, ami?)
% We implemented two approaches
% on top of Neo4j.
% Non-incremental: uses only Cypher for the pattern matching. 
%   Beside Neo4j's indexes, no additonal data structures are built.

% TODO sajat implementacio leirasa (manualis allokacio)

% Incremental: builds a distributed Rete network to support incremental pattern matching, and 
%     maintains both the databases and the Rete net upon modification. 
%     Only uses Cypher to retrieve the graph nodes for the type indexers of the Rete net. 

% For the incremental query, we manually created the Rete network and 
% generated random instance models of different sizes to benchmark the query's execution time.



% lifted from thesis work

The \tb{} consists of the following phases:

\begin{enumerate}
  \item $\mathit{read}$: loading the model,
  \item $\mathit{check}_0$: running the queries,
  \item $\mathit{edit}_i$: editing the model, 
  \item $\mathit{check}_i$: running the queries again.
\end{enumerate}

In a ''real-world'' model editing sequence, the user tipically edits the model in small steps ($\mathit{edit}_i$ phases). The user's work is much more productive if she receives an instant feedback, hence we would like to run re-evaluate well-formedness queries quickly (preferably in sub-second time). This creates the need for an incremental pattern matcher tools.

Measure the response time and the scalability

Workload profile's difference from standard benchmarks
