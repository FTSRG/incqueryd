\chapter{Related Work}
\label{chap:related-work}

A wide range of special languages have been developed to support \emph{graph-based} representation and querying of computer data. This chapter collects the research and development works that are related to \iqd{}.

\section{Eclipse-based Tools}

A class-diagram like modeling language is Ecore of the EMF (Eclipse Modeling Framework, discussed in \autoref{emf}), where classes, references between them and attributes of classes describe the domain. 
Extensive tooling helps the creation and transformation of such domain models. For EMF models, OCL (Object Constraint Language) is a declarative constraint description and query language that can be evaluated with the local-search based Eclipse OCL~\cite{EclipseOCL} engine. To address scalability issues, \emph{incremental} impact analysis tools~\cite{OCLIA} have been developed as extensions or alternatives to Eclipse OCL.

% \section{Semantic Web and NoSQL}

% Outside the Eclipse ecosystem, the Resource Description Framework (RDF~\cite{website:rdf_standard}) is developed to support the description of instances of the semantic web, assuming sparse, ever-growing, incomplete data. Semantic models are built up from triple statements and they can be queried using the SPARQL~\cite{SPARQL} graph pattern language with tools like Sesame~\cite{sesame} or Virtuoso~\cite{openvirtuoso}. 
% 
% Property graphs~\cite{DBLP:journals/corr/abs-1006-2361} provide a more general way to describe graphs by annotating vertices and edges with key-value properties. 
% 
% Such data structures can be stored in graph databases like Neo4j~\cite{neo4j} which provides the Cypher~\cite{cypher} query language. Even though big data storage (usually based on MapReduce) provides fast object persistence and retrieval, query engines realized directly on these data structures do not provide dedicated support for incremental query evaluation. 

\section{Rete Implementations}

%In the context of event-based systems, distributed evaluation engines were proposed earlier~\cite{message-passing-rete}. However they scaled up in the number of rules~\cite{mapreduce-rete} rather than in the number of data elements. 

As a very recent development, Rete-based caching approaches have been proposed for the processing of Linked Data (bearing the closest similarity of our approach). \mbox{INSTANS}~\cite{INSTANS2012} uses this algorithm to perform complex event processing (formulated in SPARQL) on RDF data, gathered from distributed sensors.

Diamond~\cite{miranker2012diamond} uses a \emph{distributed Rete network} to evaluate SPARQL queries on Linked Data, but it lacks an indexing middleware layer so their main challenge is efficient data traversal.

The conceptual foundations of our approach as based on \eiq{}~\cite{models10}, a tool that evaluates graph patterns over EMF models using Rete. Up to our best knowledge, \iqd{} is the first approach to promote distributed scalability by \emph{distributed incremental query evaluation} in the context of model-driven engineering. As the architecture of \iqd{} separates the data store from the query engine, we believe that the scalable processing of RDF and property graphs can open up interesting applications outside of the MDE world.

Acharya et al.\ described a Rete network mapping for fine-grained and medium-grained message-passing computers~\cite{message-passing-rete}. The medium-grained computer connected processors in a crossbar architecture, while our approach use computers connected by gigabit Ethernet. The paper published benchmark results of the medium-grained solution, but these are based only on simulations.

\section{Benchmarks}

This section is based on~\cite{ASE2013}. Benchmarks have been proposed earlier, mainly to track improvements of a query engine, or to compare tool performance for a given use case. However, most benchmarks are only useful for predicting (relative) performance of tools depending on model size; there is rarely enough data to consider other signals such as query or model structure. Despite the abundance of benchmarks, it is still difficult to choose the best tool for a given purpose, due to the absence of common
metrics.

\subsection{RDF Benchmarks}

SP$^{2}$Bench~\cite{SP2Bench} is a SPARQL benchmark that measures only
query throughput. The goal of this benchmark is to measure query evaluating
performance of different tools for a single set of SPARQL queries that contain most language
elements. The artificially generated data is based on the real world
DBLP bibliography; this way instance models of different sizes reflect the
structure and complexity of the original real world dataset. However, other
model element distributions or queries were not considered, and the complexity of queries
were not analyzed.

The Berlin SPARQL Benchmark (BSBM)~\cite{BSBM} measures SPARQL query evaluation throughput for an
e-commerce case study modeled in RDF. The benchmark uses a single dataset, but recognizes several use cases with their own query mix. The dataset scales in model size (10M-150B), but does not vary in structure.  

SPLODGE~\cite{SPLODGE} is an approach, where SPARQL queries were generated
systematically, based on metrics for a predefined dataset. The method supports distributed
SPARQL queries (via the \emph{SERVICE} keyword), however the implementation
scaled only up to three steps of navigation, due to the resource consumption of the
generator. The paper did not mention instance model complexity, and only the
adequacy of the approach was demonstrated with the RDF3X engine, the effect of
queries with different metrics combinations to different engines was not tested.

\subsection{Model Transformation and Graph Transformation Benchmarks}

There are numerous graph transformation benchmarks that do not focus specifically on query performance. However \cite{icgt08-bhrv} aims to design and evaluate graph transformation benchmark cases corresponding to three usage patterns for the purpose of measuring the performance of incremental approaches on different model sizes and workloads. These scenarios are conceptual continuations of the comprehensive graph transformation benchmark library proposed earlier in \cite{vlhcc05_vsv} (described more extensively in \cite{VSV05}), which gave an overview on typical application scenarios of graph transformation together with their characteristic features. \cite{GK:07} suggested some improvements to the benchmarks described in \cite{vlhcc05_vsv} and reported measurement results for many graph transformation tools. 

A similar approach to graph transformation benchmarking was used for the AGTIVE Tool Contest~\cite{AGTIVEToolContest}, including a simulation problem for the Ludo table game. %The Petri net firing test case that was presented in~\cite{icgt08-bhrv} is better suited for benchmarking performance since it can be parameterized to scale up to large model sizes and long transformation sequences. 
Later, the GraBaTs tool contest~\cite{GraBaTs08Contest} introduced an AntWorld case study~\cite{zuendorf08antworldcase}, %which had similar properties. T
and the community continued to hold tool contests in the TTC~\cite{TTC} series.

As model validation is an important use case of incremental model queries, several model query and/or validation tools have been measured in incremental constraint validation benchmarks~\cite{Egyed-incConsistency,falleri-praxis,models10}. 
